{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "### original file\n",
    "\n",
    "https://github.com/openai/gpt-2-output-dataset/blob/master/baseline.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook\n",
    "\n",
    "1. Create /output folder\n",
    "   1. Insert all crawled dataset(csv)\n",
    "   1. Rename them as same with GPT dataset files.\n",
    "1. Create /log folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "# example code from https://github.com/SKT-AI/KoGPT2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data + preprocessing\n",
    "\n",
    "def load_data(data_dir, crawled_dir, source):\n",
    "    path = os.path.join(data_dir, \"{}.csv\".format(source))\n",
    "    crawled_path = os.path.join(crawled_dir, \"{}.csv\".format(source))\n",
    "    dataset = list(csv.reader(open(path, encoding=\"utf8\")))\n",
    "    crawled_dataset = list(csv.reader(open(crawled_path, encoding=\"cp949\")))\n",
    "    n = len(dataset)\n",
    "\n",
    "    texts = []\n",
    "    labels = [1, 0] * n\n",
    "\n",
    "    for data in dataset:\n",
    "        idx = int(data[0])\n",
    "        tokens = tokenizer.tokenize(data[5])\n",
    "        texts.append(' '.join(tokens))\n",
    "        tokens = tokenizer.tokenize(crawled_dataset[idx][4])\n",
    "        texts.append(' '.join(tokens))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "    train_test_ratio=0.1,\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        texts_list, labels_list, test_size=train_test_ratio, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    vect = TfidfVectorizer()\n",
    "    train_features = vect.fit_transform(texts_train)\n",
    "    test_features = vect.transform(texts_test)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_features, labels_train)\n",
    "    test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "    result_proba = model.predict_proba(test_features)\n",
    "    result_log_proba = model.predict_log_proba(test_features)\n",
    "    ce_loss = 0\n",
    "    for label, value in zip(labels_test, result_log_proba):\n",
    "        ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "    ce_loss /= len(labels_test)\n",
    "    data = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "        \"ce_loss\": ce_loss,\n",
    "        \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "    }\n",
    "    print(data)\n",
    "    json.dump(data, open(os.path.join(log_dir, \"result.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 89.0, 'mse_loss': 0.06409735769225443, 'ce_loss': 0.4115393467022112, 'label_and_result': [(1, 1, 0.8938862220310037), (0, 0, 0.29983007310361404), (0, 0, 0.06266261655809384), (0, 1, 0.5104705335872773), (0, 0, 0.3029276663974534), (0, 0, 0.1347392069963472), (1, 1, 0.8787472389132784), (0, 0, 0.24742057246360494), (0, 0, 0.3319162892057374), (0, 0, 0.1361163372521789), (1, 1, 0.7230144092337164), (1, 1, 0.8319886420089383), (1, 1, 0.8591986159723503), (1, 1, 0.723666666057942), (1, 1, 0.5917357542514665), (0, 0, 0.15507577016178148), (1, 1, 0.7214305531150553), (1, 1, 0.6771789012269076), (1, 0, 0.47951477865261183), (1, 1, 0.6711254036996299), (1, 1, 0.7386367912575289), (1, 1, 0.6106676600605854), (0, 0, 0.4935376953822147), (0, 1, 0.608762890852627), (1, 0, 0.49901993175276893), (0, 0, 0.2917630465955634), (0, 0, 0.3330666883798451), (1, 1, 0.7345719456572994), (1, 1, 0.5716164115961246), (0, 1, 0.5782218005576287), (1, 1, 0.7980811654962486), (1, 1, 0.7561865181873033), (1, 1, 0.6898400116309643), (0, 0, 0.40199465774583987), (1, 1, 0.7316157128882378), (1, 1, 0.6286778277802918), (0, 0, 0.15677150880380386), (1, 0, 0.44919987457411087), (1, 0, 0.08618678607610676), (1, 1, 0.7847775260107811), (1, 1, 0.546886563544549), (0, 0, 0.13512261028649644), (0, 1, 0.6466247931984261), (0, 0, 0.39491725716311354), (1, 1, 0.7711378909389413), (1, 1, 0.7529385890023125), (0, 0, 0.451243292191339), (0, 0, 0.3345000411730541), (1, 1, 0.7107828334398826), (1, 1, 0.875725762129572), (1, 1, 0.6223926279266414), (0, 1, 0.5031430594713586), (1, 1, 0.7284406428989003), (1, 1, 0.6216543304445755), (0, 0, 0.08483038857513397), (0, 0, 0.3457220900210506), (1, 1, 0.5838130906342308), (1, 1, 0.8502153351431835), (1, 1, 0.7310190441107046), (0, 0, 0.37981003922511464), (0, 0, 0.05392923001419315), (1, 1, 0.6947715792290263), (0, 0, 0.4396084861337145), (0, 0, 0.2472506565579303), (0, 0, 0.412057609741354), (0, 0, 0.11581280352514933), (0, 0, 0.3896331364907858), (1, 1, 0.67898946258329), (0, 0, 0.39385572943841973), (0, 0, 0.07153658542160214), (0, 0, 0.4093508561148703), (0, 0, 0.392079139126135), (0, 0, 0.48658985298296026), (1, 1, 0.8385180376283843), (0, 0, 0.4794512348600142), (1, 1, 0.7317536202143887), (0, 0, 0.34805657058411454), (0, 0, 0.28244772261664675), (1, 1, 0.7920438906295568), (0, 0, 0.26295466698044656), (0, 0, 0.1495780490640075), (0, 0, 0.36150359091094314), (0, 0, 0.3954312104356971), (0, 0, 0.1291681679235454), (0, 0, 0.2066355272958217), (1, 1, 0.6227287767215856), (1, 1, 0.7016952626657866), (1, 1, 0.8153214174329294), (0, 0, 0.2485009950615617), (0, 0, 0.3048445945491083), (1, 1, 0.5816039992885013), (0, 0, 0.14948876166039546), (1, 1, 0.8626284122268268), (0, 0, 0.457905552590159), (1, 1, 0.8169035471074649), (0, 0, 0.12796231227674654), (0, 0, 0.4346485640314346), (0, 0, 0.2798335868730975), (1, 1, 0.7862942356717307), (0, 0, 0.1111846002446763), (1, 1, 0.7931796526239353), (0, 0, 0.26761152105750136), (0, 0, 0.44599433749225104), (1, 1, 0.5251767894007563), (0, 0, 0.45269404758527276), (0, 0, 0.40625000652616206), (1, 1, 0.7502316883419561), (1, 0, 0.4917697303198858), (0, 0, 0.3094534467663386), (1, 1, 0.6274290383206748), (0, 0, 0.30347295326904533), (0, 0, 0.160225502572126), (0, 0, 0.17711767232689182), (0, 0, 0.09320126734839532), (0, 0, 0.1586224330029638), (1, 0, 0.46616748497403), (0, 0, 0.3544412653037256), (0, 1, 0.5521637273592619), (1, 1, 0.6755793834338618), (0, 0, 0.21005312979380955), (0, 0, 0.3022471646527749), (1, 1, 0.5535853404154438), (0, 0, 0.456620561520012), (1, 1, 0.8125540641096571), (0, 0, 0.21477262972319894), (1, 0, 0.4183617794465379), (1, 1, 0.7433036293840962), (1, 1, 0.6638310664438044), (0, 0, 0.39574044713094864), (1, 1, 0.8931524799063493), (0, 0, 0.30726558726513803), (0, 0, 0.1877708428860549), (1, 1, 0.6069893789884144), (0, 1, 0.5012313739771513), (1, 1, 0.7988632174356443), (0, 0, 0.19177342992731158), (1, 1, 0.6041278577476086), (0, 0, 0.24802511164444302), (0, 1, 0.5763240934784984), (1, 1, 0.5006129055442357), (0, 0, 0.1515120656645699), (1, 1, 0.6696578173978058), (1, 1, 0.6024721870909022), (1, 1, 0.897195170011117), (1, 1, 0.8630287278444493), (1, 1, 0.5815082242360075), (1, 1, 0.7408801698161294), (0, 0, 0.4458976356878402), (0, 0, 0.4586561431876716), (1, 1, 0.6092059768295852), (0, 1, 0.6504443350739088), (0, 0, 0.06933157716451081), (0, 0, 0.2594975950722136), (1, 1, 0.6638577022155376), (0, 0, 0.4597308592365603), (0, 0, 0.10275036269995895), (1, 1, 0.929498660512692), (0, 1, 0.5331056727535621), (0, 0, 0.4192767531977588), (1, 1, 0.8801830261884722), (1, 1, 0.9284628635834413), (1, 1, 0.6910581669795923), (0, 0, 0.12777829980541505), (1, 1, 0.7662679875851761), (0, 0, 0.4343226175777041), (1, 0, 0.441898958692841), (1, 1, 0.649000372932572), (1, 1, 0.734189712276569), (0, 0, 0.16138605883781632), (0, 1, 0.532501997538305), (1, 0, 0.46405648404950955), (0, 0, 0.30465427293401515), (1, 1, 0.582929522452747), (1, 1, 0.826020460074341), (0, 0, 0.4526087437350802), (1, 1, 0.8558439297136868), (1, 1, 0.5765822407096519), (0, 0, 0.47600160777115874), (1, 1, 0.8869471961008143), (1, 1, 0.5102177497740645), (1, 1, 0.836505102949959), (0, 0, 0.4272105120550023), (1, 1, 0.616855248781843), (1, 1, 0.699238790999388), (0, 0, 0.3653865334672451), (1, 1, 0.8256209747475003), (1, 1, 0.739407289285256), (0, 1, 0.5282945747442359), (1, 1, 0.593301543199052), (0, 0, 0.43664220719827934), (1, 1, 0.553579889053162), (1, 1, 0.7199442574507998), (1, 1, 0.697613457995912), (0, 0, 0.2636397315720052), (0, 0, 0.2891455518797829), (1, 1, 0.5071194928287484), (0, 0, 0.35970816918617277), (0, 0, 0.4242848613811434), (0, 0, 0.28043973274065676), (0, 1, 0.5167680755513773)]}\n"
     ]
    }
   ],
   "source": [
    "# run main function\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
