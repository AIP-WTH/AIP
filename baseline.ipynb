{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "### original file\n",
    "\n",
    "https://github.com/openai/gpt-2-output-dataset/blob/master/baseline.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook\n",
    "\n",
    "1. Create /output folder\n",
    "   1. Insert all crawled dataset(csv)\n",
    "   1. Rename them as same with GPT dataset files.\n",
    "1. Create /log folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "# example code from https://github.com/SKT-AI/KoGPT2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data + preprocessing\n",
    "\n",
    "def load_data(data_dir, crawled_dir, source):\n",
    "    path = os.path.join(data_dir, \"{}.csv\".format(source))\n",
    "    crawled_path = os.path.join(crawled_dir, \"{}.csv\".format(source))\n",
    "    dataset = list(csv.reader(open(path, encoding=\"utf8\")))\n",
    "    crawled_dataset = list(csv.reader(open(crawled_path, encoding=\"cp949\")))\n",
    "    n = len(dataset)\n",
    "\n",
    "    texts = []\n",
    "    labels = [1, 0] * n\n",
    "\n",
    "    for data in dataset:\n",
    "        idx = int(round(float(data[0])))\n",
    "        tokens = tokenizer.tokenize(data[5])\n",
    "        texts.append(' '.join(tokens))\n",
    "        tokens = tokenizer.tokenize(crawled_dataset[idx][4])\n",
    "        texts.append(' '.join(tokens))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "    train_test_ratio=0.1,\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        texts_list, labels_list, test_size=train_test_ratio, random_state=42, shuffle=True,\n",
    "    )\n",
    "\n",
    "    vect = TfidfVectorizer()\n",
    "    train_features = vect.fit_transform(texts_train)\n",
    "    test_features = vect.transform(texts_test)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(train_features, labels_train)\n",
    "    test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "    result = model.predict(test_features)\n",
    "    result_proba = model.predict_proba(test_features)\n",
    "    result_log_proba = model.predict_log_proba(test_features)\n",
    "    kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "    for res, pred in zip(result, labels_test):\n",
    "        if res == 1:\n",
    "            kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "        else:\n",
    "            kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "    precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "    recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "    ce_loss = 0\n",
    "    for label, value in zip(labels_test, result_log_proba):\n",
    "        ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "    ce_loss /= len(labels_test)\n",
    "    data = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "        \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "        \"ce_loss\": ce_loss,\n",
    "        \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "    }\n",
    "    print(data)\n",
    "    json.dump(data, open(os.path.join(log_dir, \"result.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 79.5, 'test_precision': 0.8743169398907104, 'test_recall': 0.730593607305936, 'F_score': 0.7960199004975124, 'mse_loss': 0.40459746344583253, 'ce_loss': 0.45505534854078955, 'label_and_result': [(0, 0, 0.07634600128452455), (0, 0, 0.16504851450386412), (0, 0, 0.2444029402884383), (0, 0, 0.2563408869757613), (0, 0, 0.2982962573824878), (1, 1, 0.7590905311186416), (1, 0, 0.356134738947665), (0, 0, 0.14286551545305487), (1, 1, 0.7918421963086042), (0, 0, 0.15100251374555895), (0, 0, 0.2612098532838189), (1, 1, 0.894558528985722), (1, 1, 0.8647948925509544), (1, 1, 0.8594827531997338), (1, 1, 0.7215471811569294), (1, 1, 0.7233544962965331), (0, 0, 0.11907933192061836), (0, 1, 0.5268845189553784), (1, 0, 0.44002152342780154), (0, 0, 0.43262414549413886), (0, 0, 0.3895782946161929), (1, 1, 0.8000492477760879), (1, 1, 0.7864625514791802), (0, 0, 0.2863972943921329), (1, 1, 0.8757810997177561), (0, 0, 0.3364349082305788), (0, 0, 0.09852255950266682), (1, 1, 0.5821288216146966), (0, 0, 0.46141679261878255), (0, 0, 0.3313017780787025), (0, 0, 0.34973039081814433), (1, 1, 0.9793997728609756), (1, 1, 0.875091860844823), (1, 0, 0.4427987983324767), (0, 0, 0.41313707480695583), (0, 0, 0.3903314981555293), (0, 0, 0.4444859991643899), (1, 0, 0.368507109736657), (1, 0, 0.38454226161037836), (0, 0, 0.1928089961753591), (1, 0, 0.3188759985034293), (1, 1, 0.6147250908222822), (0, 0, 0.04514944102023015), (0, 1, 0.5332299411712266), (0, 0, 0.1453537017233169), (1, 1, 0.7161314362854669), (0, 1, 0.5597417589774122), (0, 0, 0.48695607151564607), (0, 0, 0.2928694886024777), (1, 1, 0.8657551361317136), (0, 0, 0.07586839417819553), (1, 1, 0.7311116984331376), (1, 1, 0.8188874539789129), (1, 0, 0.23824070461491428), (0, 0, 0.4027645209268265), (0, 0, 0.3792734600471861), (1, 0, 0.35866394564733184), (0, 0, 0.3204086268575969), (1, 1, 0.6752989897667611), (1, 1, 0.7792267904817157), (0, 0, 0.16284846594326854), (0, 0, 0.18247750974828303), (1, 1, 0.8456035019785966), (0, 0, 0.41327805553547375), (1, 0, 0.3372872263301397), (1, 1, 0.6497470842637005), (1, 1, 0.7825790207826847), (0, 0, 0.42406088700025957), (1, 0, 0.40674030490345225), (1, 1, 0.7498010525627113), (0, 1, 0.6371613946310112), (1, 1, 0.8796274574218345), (1, 1, 0.8421303094135992), (1, 1, 0.9239824306438083), (0, 0, 0.29880169545343505), (1, 1, 0.6672766480167918), (0, 0, 0.08964399859051209), (1, 1, 0.7223585940618541), (0, 0, 0.14661032757222034), (1, 1, 0.8857169319004009), (1, 0, 0.4922857850556347), (1, 0, 0.2850934295316351), (0, 0, 0.04522102154620483), (0, 0, 0.32994037889911915), (1, 1, 0.9272823750629261), (1, 1, 0.588176953787717), (0, 0, 0.31218818698909223), (0, 1, 0.612954094316689), (0, 0, 0.4164162203921465), (0, 0, 0.3852544052141533), (1, 1, 0.5399709150757366), (0, 0, 0.37419113493925904), (0, 0, 0.4831329780749515), (0, 0, 0.32024632737298386), (1, 0, 0.3286658758213516), (1, 1, 0.8562849993818112), (1, 1, 0.6664476920862814), (1, 1, 0.6440951044185819), (1, 1, 0.7278119488519803), (1, 0, 0.4961826498796493), (0, 0, 0.2735800063988441), (0, 1, 0.5407148947750715), (0, 1, 0.5769452528791759), (1, 1, 0.688777141407023), (1, 0, 0.4757392687503483), (1, 1, 0.9180370284616004), (0, 0, 0.25552699774231824), (1, 1, 0.5134119630518333), (1, 1, 0.6900885234435751), (1, 1, 0.9689128363670821), (0, 0, 0.29005143027076485), (1, 0, 0.3227081573829898), (0, 0, 0.25496453371075567), (0, 0, 0.29147558373073196), (1, 1, 0.7865974659897015), (0, 1, 0.6062664569132151), (0, 0, 0.4880872934329182), (1, 1, 0.9676635723802153), (1, 1, 0.974245500664984), (0, 0, 0.16643063393550164), (1, 0, 0.3858437965090872), (1, 0, 0.3317191621184049), (1, 1, 0.8587827485435114), (1, 1, 0.5298748739154693), (1, 1, 0.8098378171628652), (1, 1, 0.7586827198653219), (1, 0, 0.3198498321569168), (0, 0, 0.09944288921367792), (1, 0, 0.438788752327345), (1, 1, 0.5238022844782866), (0, 1, 0.526285281093145), (1, 1, 0.8801740937616543), (1, 0, 0.3159750065287383), (1, 1, 0.5236962735870521), (1, 1, 0.8765015343924971), (0, 0, 0.3317191621184049), (0, 0, 0.053074415166483396), (0, 0, 0.3781607684006336), (0, 1, 0.640828206675472), (0, 0, 0.3858437965090872), (0, 1, 0.5199150346024518), (0, 0, 0.47733418805096184), (1, 1, 0.7086312205456984), (1, 1, 0.836610202159044), (1, 0, 0.4920335894573517), (1, 0, 0.328441342373613), (0, 0, 0.325798126965867), (0, 0, 0.3615626059605524), (1, 1, 0.5853023492991863), (1, 1, 0.9206331306677245), (1, 0, 0.2336107109916501), (0, 0, 0.4429169968522182), (1, 1, 0.7632058766376477), (0, 1, 0.5326402557602514), (1, 1, 0.9353386494431294), (1, 1, 0.8849662678561581), (1, 1, 0.9359896041747857), (0, 0, 0.14051435340842747), (0, 0, 0.2707070879859184), (1, 0, 0.22894245620840653), (1, 1, 0.698678950434721), (0, 0, 0.1822547420321807), (1, 0, 0.37419113493925904), (1, 0, 0.2972337193665959), (0, 0, 0.3494209400184656), (1, 1, 0.7073094061078217), (1, 0, 0.4649265753092471), (0, 0, 0.4507170136964414), (0, 0, 0.25231152393002976), (0, 0, 0.09373709803018795), (1, 1, 0.8961548818540763), (0, 0, 0.4718899065087499), (1, 1, 0.6010883066704856), (0, 0, 0.27927568772604416), (0, 0, 0.25500897163529523), (0, 0, 0.39520583203145476), (0, 1, 0.5013672264941185), (1, 1, 0.5456141885524951), (0, 0, 0.44300064538669987), (0, 0, 0.20172324206559458), (1, 0, 0.44300064538669987), (0, 0, 0.4489370394695991), (1, 1, 0.9446391895806974), (0, 0, 0.23982161211583714), (1, 1, 0.6536612219821648), (1, 1, 0.9587120533863178), (0, 0, 0.22978958053656617), (0, 1, 0.5551550767500834), (0, 0, 0.2916625740715972), (1, 1, 0.8005680577533442), (0, 0, 0.4276278101777446), (1, 1, 0.9130111408859085), (1, 1, 0.906062911998393), (1, 1, 0.9001296777059029), (1, 1, 0.6279806302747308), (1, 1, 0.6898665476268959), (1, 1, 0.9798979359753548), (0, 0, 0.34266855784311157), (1, 1, 0.9109570455476313), (0, 0, 0.35997094123302775), (1, 0, 0.4169306102369465), (1, 1, 0.8070820082715336), (0, 1, 0.6875034006369733), (0, 0, 0.19306510264819532), (1, 1, 0.7159283707722783), (0, 0, 0.3496814332970031), (0, 1, 0.5057955405917073), (1, 1, 0.8829176428350267), (0, 0, 0.0536416531852261), (0, 1, 0.7726456468223776), (1, 1, 0.6212349211607245), (0, 0, 0.2907537107406828), (0, 0, 0.36490248848218665), (1, 0, 0.2738217527764154), (1, 1, 0.7987580072086056), (0, 0, 0.19022291228087027), (1, 1, 0.7685406462894072), (0, 0, 0.27799762106011366), (1, 1, 0.8550257874370604), (0, 0, 0.09398841839994346), (0, 0, 0.4360874751983791), (0, 1, 0.5228751172943719), (0, 0, 0.46485721536129343), (1, 1, 0.8020184037038234), (1, 0, 0.41313707480695583), (1, 1, 0.8631470257503889), (1, 1, 0.8174991535191777), (0, 0, 0.3148373525151086), (1, 0, 0.23609584602073191), (1, 1, 0.6900315557395658), (1, 1, 0.847831008281099), (1, 1, 0.7661967469698264), (1, 0, 0.37023807612987275), (0, 0, 0.45896624874704456), (1, 1, 0.5675111917653278), (0, 0, 0.22509849582675517), (0, 0, 0.3433506753238212), (1, 1, 0.6617546640789025), (0, 0, 0.22274092224281042), (0, 0, 0.42536228555935873), (0, 0, 0.4573054942672976), (0, 0, 0.09511187737659454), (1, 1, 0.7901901759003349), (1, 1, 0.872729222180452), (1, 1, 0.898821891585564), (0, 0, 0.09156523818602952), (0, 0, 0.09476376867015476), (1, 1, 0.6071014957399085), (0, 1, 0.5412447533744521), (1, 1, 0.5977452753109956), (1, 1, 0.8781431695017767), (0, 0, 0.2946718230204409), (0, 0, 0.390170439915746), (1, 1, 0.7342573017802059), (1, 0, 0.3167819870247858), (1, 1, 0.81761893807954), (1, 1, 0.9576567227241305), (1, 1, 0.870985339159205), (1, 1, 0.8840198777363123), (0, 0, 0.45639622078926556), (1, 1, 0.6891462174865037), (1, 1, 0.893236364083718), (0, 0, 0.3726994304708511), (1, 0, 0.35761881970912374), (0, 0, 0.364591742952263), (0, 0, 0.42019450440992295), (0, 0, 0.10163128482213823), (0, 0, 0.258547113558408), (0, 0, 0.11726494066562156), (1, 1, 0.9065800428694433), (1, 1, 0.8696708202422057), (0, 0, 0.11748542116335586), (0, 0, 0.46904395929019527), (0, 1, 0.7812605141140176), (0, 0, 0.41263372398732784), (1, 1, 0.8758359874872342), (0, 0, 0.061948254374520256), (0, 0, 0.31549894842368204), (0, 0, 0.4611720354879819), (1, 0, 0.4964212564527429), (0, 0, 0.39612942170583093), (1, 1, 0.7684473954907545), (0, 0, 0.3823333227653635), (1, 0, 0.4510667487158217), (1, 0, 0.3695132656531506), (0, 0, 0.14703881775391517), (1, 1, 0.8950499564231232), (0, 0, 0.3208139662569503), (1, 1, 0.6911419551825638), (1, 1, 0.69135420741946), (1, 1, 0.6920485846170293), (1, 1, 0.8739033125543777), (0, 0, 0.3391923442707382), (1, 1, 0.7037640513820882), (1, 1, 0.7655276971356552), (1, 1, 0.7379436372268645), (0, 0, 0.2884772390358228), (0, 0, 0.4893576195331013), (0, 0, 0.3375925654184749), (1, 0, 0.49176844095335126), (1, 1, 0.7015588155307438), (1, 1, 0.5230344001667152), (1, 1, 0.5801102755073415), (1, 1, 0.9674045635811493), (0, 0, 0.3003680347395384), (1, 0, 0.41602075160661295), (1, 1, 0.673982935147878), (0, 0, 0.4075076219671565), (0, 0, 0.21924064482564443), (0, 0, 0.40445958598516374), (1, 0, 0.49028829191414314), (1, 1, 0.532935295229242), (0, 0, 0.4418414673749188), (1, 1, 0.9445868601398315), (0, 0, 0.07676407122774015), (1, 1, 0.7552128551259836), (0, 0, 0.2743291403544052), (1, 1, 0.509883935462347), (1, 0, 0.3865200079527238), (1, 1, 0.9714578880508999), (0, 0, 0.25986587468758743), (1, 0, 0.2534760794784831), (1, 1, 0.8776787311837777), (0, 0, 0.3135880612151844), (0, 0, 0.42074098095246837), (0, 0, 0.17533557805676137), (1, 0, 0.36636039329235026), (1, 1, 0.9068173700729156), (0, 0, 0.2274499533450242), (1, 1, 0.7972326799569044), (1, 1, 0.6989039811805564), (0, 0, 0.460450023799851), (1, 1, 0.9138932183184842), (1, 1, 0.7596485147259033), (1, 1, 0.8629133163674516), (0, 0, 0.44221202344674465), (1, 1, 0.9245786142342363), (0, 0, 0.4975264532716139), (1, 1, 0.514833521842056), (1, 0, 0.35769546314534884), (1, 0, 0.21476732812604565), (1, 1, 0.6697568925607014), (1, 1, 0.7946207062883552), (0, 0, 0.41616935633455265), (1, 0, 0.3554135856295363), (1, 0, 0.323626518453421), (1, 1, 0.8754798717207999), (1, 0, 0.052056131710257164), (1, 0, 0.3721960562732267), (0, 0, 0.32978275968207676), (0, 0, 0.4525133031106789), (0, 0, 0.38188646638265344), (0, 0, 0.32811155221172467), (1, 1, 0.7339505939707625), (1, 1, 0.847099300414835), (0, 0, 0.37041758540114894), (1, 1, 0.8065599454470103), (0, 0, 0.19486072179819183), (0, 0, 0.24128718824961728), (1, 0, 0.2891152346321829), (0, 0, 0.37471434109551255), (0, 0, 0.45739813709886623), (1, 1, 0.8997377153795156), (1, 1, 0.9072526736319432), (1, 1, 0.5460127683692086), (1, 0, 0.348760488056788), (0, 0, 0.04951313093248685), (0, 1, 0.5253675398219305), (0, 0, 0.3583287733587907), (1, 1, 0.8090395639026964), (1, 1, 0.6964089073509023), (1, 0, 0.34976136624842424), (1, 1, 0.6782509062997146), (0, 0, 0.1219391653485218), (1, 0, 0.46850067316764493), (0, 0, 0.3252974000252377), (1, 0, 0.38437401098892593), (1, 1, 0.789513632651441), (1, 0, 0.36943241220848405), (1, 1, 0.7723364252036706), (1, 1, 0.9773605383586534), (1, 1, 0.8249610323672414), (1, 1, 0.8512580115691537), (0, 0, 0.16430205016076319), (1, 1, 0.768020246601761), (1, 1, 0.5846270467481413), (1, 1, 0.8749252366229326), (1, 1, 0.7834212302192021), (0, 0, 0.11436045315111797), (0, 0, 0.19298907149662775), (1, 0, 0.44423983933252537), (0, 0, 0.24394885039468886), (1, 1, 0.5447860308620511), (1, 1, 0.9736489234133131), (0, 1, 0.5765312847158568), (1, 0, 0.2503939142145301), (0, 0, 0.2804793014293037), (1, 1, 0.9775793070853138), (0, 1, 0.5320893240772065), (0, 0, 0.09448780851683485)]}\n"
     ]
    }
   ],
   "source": [
    "# run main function\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
