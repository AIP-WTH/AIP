{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "\n",
    "### original file\n",
    "\n",
    "https://github.com/openai/gpt-2-output-dataset/blob/master/baseline.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook\n",
    "\n",
    "1. Create /output folder\n",
    "   1. Insert all crawled dataset(csv)\n",
    "   1. Rename them as same with GPT dataset files.\n",
    "1. Create /log folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between baseline\n",
    "\n",
    "- Using Word2Vec instead of TF-IDF Vectorizer\n",
    "- Using Gridsearch to find best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "# example code from https://github.com/SKT-AI/KoGPT2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data + preprocessing\n",
    "\n",
    "def load_data(data_dir, crawled_dir, source):\n",
    "    path = os.path.join(data_dir, \"{}.csv\".format(source))\n",
    "    crawled_path = os.path.join(crawled_dir, \"{}.csv\".format(source))\n",
    "    dataset = list(csv.reader(open(path, encoding=\"utf8\")))\n",
    "    crawled_dataset = list(csv.reader(open(crawled_path, encoding=\"cp949\")))\n",
    "    n = len(dataset)\n",
    "    \n",
    "    length = min(50, n) if SHORT_MODE else n\n",
    "\n",
    "    texts = []\n",
    "    labels = [1, 0] * length\n",
    "\n",
    "    for data in dataset[:length]:\n",
    "        idx = int(round(float(data[0])))\n",
    "        tokens = tokenizer.tokenize(data[5])\n",
    "        texts.append(' '.join(tokens))\n",
    "        tokens = tokenizer.tokenize(crawled_dataset[idx][4])\n",
    "        texts.append(' '.join(tokens))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "    train_test_ratio=0.1,\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        texts_list, labels_list, test_size=train_test_ratio, random_state=42, shuffle=True,\n",
    "    )\n",
    "    n_train, n_test = len(texts_train), len(texts_test)\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=texts_list, vector_size=512, window=9, min_count=5, workers=4, sg=0)\n",
    "    train_features = [w2v_model.wv.get_mean_vector(y) for y in texts_train]\n",
    "    test_features = [w2v_model.wv.get_mean_vector(y) for y in texts_test]\n",
    "\n",
    "    model = LogisticRegression(max_iter=100000)\n",
    "    params = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "    split = PredefinedSplit([-1]*n_train+[0]*n_test)\n",
    "    search = GridSearchCV(model, params, cv=split, refit=False)\n",
    "    search.fit(sparse.vstack([train_features, test_features]), labels_train + labels_test)\n",
    "    model = model.set_params(**search.best_params_)\n",
    "    model.fit(train_features, labels_train)\n",
    "    test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "    result = model.predict(test_features)\n",
    "    result_proba = model.predict_proba(test_features)\n",
    "    result_log_proba = model.predict_log_proba(test_features)\n",
    "    kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "    for res, pred in zip(result, labels_test):\n",
    "        if res == 1:\n",
    "            kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "        else:\n",
    "            kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "    precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "    recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "    ce_loss = 0\n",
    "    for label, value in zip(labels_test, result_log_proba):\n",
    "        ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "    ce_loss /= len(labels_test)\n",
    "    data = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "        \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "        \"ce_loss\": ce_loss,\n",
    "        \"param\": search.best_params_,\n",
    "        \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "    }\n",
    "    print(data)\n",
    "    json.dump(data, open(os.path.join(log_dir, \"result.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 86.66666666666667, 'test_precision': 0.8846153846153846, 'test_recall': 0.8214285714285714, 'F_score': 0.8518518518518519, 'mse_loss': 0.009032847198068643, 'ce_loss': 0.3901438404922017, 'param': {'C': 32}, 'label_and_result': [(1, 1, 0.5220390441391777), (0, 0, 0.27780202346772925), (0, 0, 0.19010829523195444), (0, 0, 0.036286407124581324), (0, 0, 0.31283326407949036), (1, 1, 0.8557177160855992), (1, 1, 0.5706967561540156), (0, 0, 0.3858658115295685), (1, 1, 0.8623425795253546), (0, 0, 0.04365645630591507), (0, 0, 0.1710444105191018), (1, 0, 0.2812877871917825), (0, 0, 0.37724226909525094), (1, 1, 0.9243185904469908), (0, 0, 0.106459015791026), (1, 1, 0.6549948466342912), (1, 1, 0.6426966882101939), (1, 1, 0.8084979214455724), (1, 1, 0.8210372259403137), (1, 0, 0.28243457582245884), (1, 1, 0.6029871131871425), (0, 0, 0.1586683598355534), (0, 0, 0.23769169997512996), (1, 1, 0.9168999718960049), (1, 0, 0.4583817905211404), (0, 0, 0.31885253449872386), (1, 1, 0.8747115560516238), (0, 1, 0.5001596482923728), (1, 1, 0.8249672292120197), (0, 0, 0.48343352486164226), (0, 0, 0.11386664721441656), (0, 0, 0.20794676655687497), (1, 1, 0.8370307729905659), (0, 1, 0.5713501288011609), (1, 0, 0.4453569289774618), (0, 0, 0.34721104149484355), (1, 1, 0.536002627396662), (0, 0, 0.2710088538953533), (0, 0, 0.03460134127316466), (0, 0, 0.18558269898597546), (1, 1, 0.7111534417517479), (0, 0, 0.4938224525294214), (1, 1, 0.9191565819388238), (1, 1, 0.9265263918888802), (1, 1, 0.7181106044417279), (1, 1, 0.8743915400491266), (0, 0, 0.15981199168867008), (1, 1, 0.5826796343085917), (0, 0, 0.07022803422459849), (1, 1, 0.6817917803517193), (0, 0, 0.12688827246847956), (1, 0, 0.10060311760101961), (0, 0, 0.063538225399952), (0, 0, 0.33062668788746524), (0, 0, 0.36994064186311093), (1, 1, 0.8155675519624069), (0, 1, 0.5590399659031583), (0, 0, 0.11677705170959021), (0, 0, 0.13336617466544323), (0, 0, 0.4557202539404316)]}\n"
     ]
    }
   ],
   "source": [
    "# run main function\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
