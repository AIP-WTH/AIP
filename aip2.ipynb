{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "\n",
    "### original file\n",
    "\n",
    "https://github.com/openai/gpt-2-output-dataset/blob/master/baseline.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook\n",
    "\n",
    "1. Create /output folder\n",
    "   1. Insert all crawled dataset(csv)\n",
    "   1. Rename them as same with GPT dataset files.\n",
    "1. Create /log folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between baseline\n",
    "\n",
    "- Using Word2Vec instead of TF-IDF Vectorizer\n",
    "- ~~Using Gridsearch to find best parameter~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "# example code from https://github.com/SKT-AI/KoGPT2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data + preprocessing\n",
    "\n",
    "def load_data(data_dir, crawled_dir, source):\n",
    "    path = os.path.join(data_dir, \"{}.csv\".format(source))\n",
    "    crawled_path = os.path.join(crawled_dir, \"{}.csv\".format(source))\n",
    "    dataset = list(csv.reader(open(path, encoding=\"utf8\")))\n",
    "    crawled_dataset = list(csv.reader(open(crawled_path, encoding=\"cp949\")))\n",
    "    n = len(dataset)\n",
    "    \n",
    "    length = min(50, n) if SHORT_MODE else n\n",
    "\n",
    "    texts = []\n",
    "    labels = [1, 0] * length\n",
    "\n",
    "    for data in dataset[:length]:\n",
    "        idx = int(round(float(data[0])))\n",
    "        tokens = tokenizer.tokenize(data[5])\n",
    "        texts.append(' '.join(tokens))\n",
    "        tokens = tokenizer.tokenize(crawled_dataset[idx][4])\n",
    "        texts.append(' '.join(tokens))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "    train_test_ratio=0.1,\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        texts_list, labels_list, test_size=train_test_ratio, random_state=42, shuffle=True,\n",
    "    )\n",
    "    n_train, n_test = len(texts_train), len(texts_test)\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=texts_list, vector_size=512, window=9, min_count=5, workers=4, sg=0)\n",
    "    train_features = [w2v_model.wv.get_mean_vector(y) for y in texts_train]\n",
    "    test_features = [w2v_model.wv.get_mean_vector(y) for y in texts_test]\n",
    "\n",
    "    model = LogisticRegression(max_iter=100, penalty=None)\n",
    "    # params = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "    # split = PredefinedSplit([-1]*n_train+[0]*n_test)\n",
    "    # search = GridSearchCV(model, params, cv=split, refit=False)\n",
    "    # search.fit(sparse.vstack([train_features, test_features]), labels_train + labels_test)\n",
    "    # model = model.set_params(**search.best_params_)\n",
    "    model.fit(train_features, labels_train)\n",
    "    test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "    result = model.predict(test_features)\n",
    "    result_proba = model.predict_proba(test_features)\n",
    "    result_log_proba = model.predict_log_proba(test_features)\n",
    "    kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "    for res, pred in zip(result, labels_test):\n",
    "        if res == 1:\n",
    "            kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "        else:\n",
    "            kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "    precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "    recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "    ce_loss = 0\n",
    "    for label, value in zip(labels_test, result_log_proba):\n",
    "        ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "    ce_loss /= len(labels_test)\n",
    "    data = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "        \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "        \"ce_loss\": ce_loss,\n",
    "        # \"param\": search.best_params_,\n",
    "        \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "    }\n",
    "    print(data)\n",
    "    json.dump(data, open(os.path.join(log_dir, \"result.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main_overfit(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "    train_test_ratio=0.1,\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        texts_list, labels_list, test_size=train_test_ratio, random_state=42, shuffle=True,\n",
    "    )\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=texts_list, vector_size=512, window=9, min_count=5, workers=4, sg=0)\n",
    "    train_features = [w2v_model.wv.get_mean_vector(y) for y in texts_train]\n",
    "    test_features = [w2v_model.wv.get_mean_vector(y) for y in texts_test]\n",
    "\n",
    "    model = LogisticRegression(max_iter=10000, penalty=None)\n",
    "    model.fit(train_features, labels_train)\n",
    "    test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "    result = model.predict(test_features)\n",
    "    result_proba = model.predict_proba(test_features)\n",
    "    result_log_proba = model.predict_log_proba(test_features)\n",
    "    kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "    for res, pred in zip(result, labels_test):\n",
    "        if res == 1:\n",
    "            kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "        else:\n",
    "            kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "    precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "    recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "    ce_loss = 0\n",
    "    for label, value in zip(labels_test, result_log_proba):\n",
    "        ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "    ce_loss /= len(labels_test)\n",
    "    data = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "        \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "        \"ce_loss\": ce_loss,\n",
    "        \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "    }\n",
    "    print(data)\n",
    "    json.dump(data, open(os.path.join(log_dir, \"result_overfit.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main_kfold(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=texts_list, vector_size=512, window=9, min_count=5, workers=4, sg=0)\n",
    "    \n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    for idx, (train, test) in enumerate(kf.split(texts_list)):\n",
    "        texts_train, texts_test = [texts_list[x] for x in train], [texts_list[x] for x in test]\n",
    "        labels_train, labels_test = [labels_list[x] for x in train], [labels_list[x] for x in test]\n",
    "\n",
    "        n_train, n_test = len(train), len(test)\n",
    "        \n",
    "        train_features = [w2v_model.wv.get_mean_vector(y) for y in texts_train]\n",
    "        test_features = [w2v_model.wv.get_mean_vector(y) for y in texts_test]\n",
    "\n",
    "        model = LogisticRegression(max_iter=100, penalty=None)\n",
    "        params = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "        split = PredefinedSplit([-1]*n_train+[0]*n_test)\n",
    "        search = GridSearchCV(model, params, cv=split, refit=False)\n",
    "        search.fit(sparse.vstack([train_features, test_features]), labels_train + labels_test)\n",
    "        model = model.set_params(**search.best_params_)\n",
    "        model.fit(train_features, labels_train)\n",
    "        test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "        result = model.predict(test_features)\n",
    "        result_proba = model.predict_proba(test_features)\n",
    "        result_log_proba = model.predict_log_proba(test_features)\n",
    "        kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "        for res, pred in zip(result, labels_test):\n",
    "            if res == 1:\n",
    "                kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "            else:\n",
    "                kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "        precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "        recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "        ce_loss = 0\n",
    "        for label, value in zip(labels_test, result_log_proba):\n",
    "            ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "        ce_loss /= len(labels_test)\n",
    "        data = {\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"test_precision\": precision,\n",
    "            \"test_recall\": recall,\n",
    "            \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "            \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "            \"ce_loss\": ce_loss,\n",
    "            # \"param\": search.best_params_,\n",
    "            \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "        }\n",
    "        print(data)\n",
    "        json.dump(data, open(os.path.join(log_dir, \"result{}.json\".format(idx)), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 89.16666666666667, 'test_precision': 0.9473684210526315, 'test_recall': 0.84375, 'F_score': 0.8925619834710744, 'mse_loss': 0.35444012352634696, 'ce_loss': 0.3021937717745798, 'label_and_result': [(1, 1, 0.9717779710384378), (0, 0, 0.05764387732338032), (0, 0, 0.05758253763773014), (0, 0, 0.00011709378260378344), (0, 0, 0.10689998625846188), (1, 1, 0.9918503036207272), (1, 0, 0.16363114075239638), (0, 0, 0.11278845696993474), (1, 1, 0.5166689326057543), (0, 0, 0.0016667985830455777), (0, 0, 0.002895797770067001), (1, 0, 0.11169616995427373), (0, 0, 0.030519480357694227), (1, 1, 0.9859243212264455), (0, 0, 0.010549105097666498), (1, 1, 0.9926380361104022), (1, 1, 0.9346488503155645), (1, 1, 0.950851003355753), (1, 1, 0.954515626684332), (1, 1, 0.924166347719938), (1, 1, 0.9912134410603858), (0, 0, 0.0041674234961938976), (0, 0, 0.11495959955237615), (1, 1, 0.9999484508567833), (1, 1, 0.9084537144742204), (0, 0, 0.06987334370455268), (1, 1, 0.9958249815501198), (0, 0, 0.414274440239567), (1, 1, 0.9804651837439754), (0, 1, 0.5421547043993162), (0, 0, 0.049540388211191826), (0, 0, 0.04992853712144654), (1, 1, 0.9982312148570999), (0, 1, 0.5746404198073741), (1, 1, 0.7819061775810089), (0, 0, 0.028890401230596652), (1, 0, 0.3467532714146139), (0, 0, 0.024886736515424532), (0, 0, 0.002766626536547436), (0, 0, 0.014472888728794873), (1, 1, 0.9775048933606291), (0, 0, 0.26305327890710517), (1, 1, 0.9983405174510949), (1, 1, 0.9990336879642343), (1, 1, 0.9711618292056396), (1, 1, 0.9990244588368541), (0, 0, 0.016806668735673356), (1, 1, 0.5938987050492002), (0, 0, 0.00010407508863528597), (1, 1, 0.7650710852826771), (0, 0, 0.00036464426037497474), (1, 0, 0.00490228953639262), (0, 0, 0.0006996585316326659), (0, 0, 0.03847698489252723), (0, 0, 0.07814346733770089), (1, 1, 0.9941010652241469), (0, 0, 0.2848570030771476), (0, 0, 0.0007271147815422458), (0, 0, 0.00704440026055229), (0, 0, 0.0737851228172577), (1, 0, 0.15466588653845265), (1, 1, 0.9910697113280175), (0, 0, 0.43140128697266233), (1, 1, 0.955529088631084), (1, 1, 0.9984989608328924), (1, 1, 0.998360055902374), (0, 0, 0.004253136511693349), (0, 0, 0.12113276694136647), (0, 0, 0.003915943186346783), (1, 1, 0.9973619756099578), (1, 1, 0.9223287989557243), (1, 1, 0.9815710813430948), (0, 0, 0.0006737991951724515), (1, 1, 0.6349495884019241), (0, 0, 0.018646005836842425), (0, 0, 0.18122540430371148), (1, 1, 0.990811233887422), (1, 1, 0.9525132540778998), (0, 0, 0.0005657511377256551), (1, 1, 0.8288723846769612), (1, 0, 0.2610001369604967), (1, 1, 0.9589169827117598), (1, 0, 0.23821521770164586), (0, 0, 0.0028261481442863785), (0, 0, 0.052434612053450444), (0, 0, 0.02565844617691262), (0, 0, 0.007888333764489065), (1, 1, 0.9787046985860471), (1, 1, 0.531390850384229), (1, 1, 0.984672600846948), (0, 0, 0.2554951091838776), (1, 1, 0.9656606126767295), (1, 1, 0.6514101862251175), (1, 1, 0.9979961079274255), (0, 0, 0.0014423212332010567), (1, 1, 0.7766645190008478), (1, 1, 0.9315232261524297), (0, 0, 0.06304423281329244), (1, 1, 0.9957694218507747), (0, 1, 0.590711461812982), (1, 1, 0.998103203634707), (1, 1, 0.9412379348845687), (1, 0, 0.12713450253049535), (0, 0, 0.1330168670229722), (0, 0, 0.03324010045936348), (1, 0, 0.06822093255598773), (1, 1, 0.8916944727964341), (0, 0, 0.05790314481694605), (1, 1, 0.9930393089224534), (1, 1, 0.8674281760259895), (0, 0, 0.41306826365791455), (1, 0, 0.030355462910932343), (0, 0, 0.3484368629409868), (1, 1, 0.958200943052175), (0, 0, 0.028030825164443423), (0, 0, 0.45953705038212794), (1, 1, 0.9961342560420272), (1, 1, 0.8563164482742012), (1, 1, 0.9976740628953735), (0, 0, 0.00025240607662917473)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\w2v_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# run main function\n",
    "\n",
    "main(train_test_ratio=0.2)\n",
    "# main_overfit(train_test_ratio=0.2)\n",
    "# main_kfold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
