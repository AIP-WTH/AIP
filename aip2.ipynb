{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "\n",
    "### original file\n",
    "\n",
    "https://github.com/openai/gpt-2-output-dataset/blob/master/baseline.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before running this notebook\n",
    "\n",
    "1. Create /output folder\n",
    "   1. Insert all crawled dataset(csv)\n",
    "   1. Rename them as same with GPT dataset files.\n",
    "1. Create /log folder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between baseline\n",
    "\n",
    "- Using Word2Vec instead of TF-IDF Vectorizer\n",
    "- Using Gridsearch to find best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\w2v_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "# example code from https://github.com/SKT-AI/KoGPT2\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    bos_token=\"</s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_MODE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data + preprocessing\n",
    "\n",
    "def load_data(data_dir, crawled_dir, source):\n",
    "    path = os.path.join(data_dir, \"{}.csv\".format(source))\n",
    "    crawled_path = os.path.join(crawled_dir, \"{}.csv\".format(source))\n",
    "    dataset = list(csv.reader(open(path, encoding=\"utf8\")))\n",
    "    crawled_dataset = list(csv.reader(open(crawled_path, encoding=\"cp949\")))\n",
    "    n = len(dataset)\n",
    "    \n",
    "    length = min(50, n) if SHORT_MODE else n\n",
    "\n",
    "    texts = []\n",
    "    labels = [1, 0] * length\n",
    "\n",
    "    for data in dataset[:length]:\n",
    "        idx = int(round(float(data[0])))\n",
    "        tokens = tokenizer.tokenize(data[5])\n",
    "        texts.append(' '.join(tokens))\n",
    "        tokens = tokenizer.tokenize(crawled_dataset[idx][4])\n",
    "        texts.append(' '.join(tokens))\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "    train_test_ratio=0.1,\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        texts_list, labels_list, test_size=train_test_ratio, random_state=42, shuffle=True,\n",
    "    )\n",
    "    n_train, n_test = len(texts_train), len(texts_test)\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=texts_list, vector_size=512, window=9, min_count=5, workers=4, sg=0)\n",
    "    train_features = [w2v_model.wv.get_mean_vector(y) for y in texts_train]\n",
    "    test_features = [w2v_model.wv.get_mean_vector(y) for y in texts_test]\n",
    "\n",
    "    model = LogisticRegression(max_iter=20)\n",
    "    params = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "    split = PredefinedSplit([-1]*n_train+[0]*n_test)\n",
    "    search = GridSearchCV(model, params, cv=split, refit=False)\n",
    "    search.fit(sparse.vstack([train_features, test_features]), labels_train + labels_test)\n",
    "    model = model.set_params(**search.best_params_)\n",
    "    model.fit(train_features, labels_train)\n",
    "    test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "    result = model.predict(test_features)\n",
    "    result_proba = model.predict_proba(test_features)\n",
    "    result_log_proba = model.predict_log_proba(test_features)\n",
    "    kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "    for res, pred in zip(result, labels_test):\n",
    "        if res == 1:\n",
    "            kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "        else:\n",
    "            kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "    precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "    recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "    ce_loss = 0\n",
    "    for label, value in zip(labels_test, result_log_proba):\n",
    "        ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "    ce_loss /= len(labels_test)\n",
    "    data = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "        \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "        \"ce_loss\": ce_loss,\n",
    "        \"param\": search.best_params_,\n",
    "        \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "    }\n",
    "    print(data)\n",
    "    json.dump(data, open(os.path.join(log_dir, \"result.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main_overfit(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "    train_test_ratio=0.1,\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "\n",
    "    texts_train, texts_test, labels_train, labels_test = train_test_split(\n",
    "        texts_list, labels_list, test_size=train_test_ratio, random_state=42, shuffle=True,\n",
    "    )\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=texts_list, vector_size=512, window=9, min_count=5, workers=4, sg=0)\n",
    "    train_features = [w2v_model.wv.get_mean_vector(y) for y in texts_train]\n",
    "    test_features = [w2v_model.wv.get_mean_vector(y) for y in texts_test]\n",
    "\n",
    "    model = LogisticRegression(max_iter=10000, penalty=None)\n",
    "    model.fit(train_features, labels_train)\n",
    "    test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "    result = model.predict(test_features)\n",
    "    result_proba = model.predict_proba(test_features)\n",
    "    result_log_proba = model.predict_log_proba(test_features)\n",
    "    kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "    for res, pred in zip(result, labels_test):\n",
    "        if res == 1:\n",
    "            kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "        else:\n",
    "            kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "    precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "    recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "    ce_loss = 0\n",
    "    for label, value in zip(labels_test, result_log_proba):\n",
    "        ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "    ce_loss /= len(labels_test)\n",
    "    data = {\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "        \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "        \"ce_loss\": ce_loss,\n",
    "        \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "    }\n",
    "    print(data)\n",
    "    json.dump(data, open(os.path.join(log_dir, \"result_overfit.json\"), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "\n",
    "def main_kfold(\n",
    "    data_dir=\"data/\",\n",
    "    crawl_dir=\"output/\",\n",
    "    log_dir=\"log/\",\n",
    "    topics=[\"culture\", \"economy\", \"it_science\", \"politics\", \"society\", \"world\"],\n",
    "):\n",
    "    texts_list, labels_list = [], []\n",
    "    for topic in topics:\n",
    "        texts, labels = load_data(data_dir, crawl_dir, topic)\n",
    "        texts_list.extend(texts)\n",
    "        labels_list.extend(labels)\n",
    "    \n",
    "    w2v_model = Word2Vec(sentences=texts_list, vector_size=512, window=9, min_count=5, workers=4, sg=0)\n",
    "    \n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    for idx, (train, test) in enumerate(kf.split(texts_list)):\n",
    "        texts_train, texts_test = [texts_list[x] for x in train], [texts_list[x] for x in test]\n",
    "        labels_train, labels_test = [labels_list[x] for x in train], [labels_list[x] for x in test]\n",
    "\n",
    "        n_train, n_test = len(train), len(test)\n",
    "        \n",
    "        train_features = [w2v_model.wv.get_mean_vector(y) for y in texts_train]\n",
    "        test_features = [w2v_model.wv.get_mean_vector(y) for y in texts_test]\n",
    "\n",
    "        model = LogisticRegression(max_iter=100)\n",
    "        params = {'C': [1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4, 8, 16, 32, 64]}\n",
    "        split = PredefinedSplit([-1]*n_train+[0]*n_test)\n",
    "        search = GridSearchCV(model, params, cv=split, refit=False)\n",
    "        search.fit(sparse.vstack([train_features, test_features]), labels_train + labels_test)\n",
    "        model = model.set_params(**search.best_params_)\n",
    "        model.fit(train_features, labels_train)\n",
    "        test_accuracy = model.score(test_features, labels_test) * 100.0\n",
    "\n",
    "        result = model.predict(test_features)\n",
    "        result_proba = model.predict_proba(test_features)\n",
    "        result_log_proba = model.predict_log_proba(test_features)\n",
    "        kind = {\"tp\": 0, \"fp\": 0, \"fn\": 0, \"tn\": 0}\n",
    "        for res, pred in zip(result, labels_test):\n",
    "            if res == 1:\n",
    "                kind[\"tp\" if res == pred else \"fp\"] += 1\n",
    "            else:\n",
    "                kind[\"tn\" if res == pred else \"fn\"] += 1\n",
    "        precision = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fp\"])\n",
    "        recall = kind[\"tp\"] / (kind[\"tp\"] + kind[\"fn\"])\n",
    "\n",
    "        ce_loss = 0\n",
    "        for label, value in zip(labels_test, result_log_proba):\n",
    "            ce_loss -= label * value[1] + (1 - label) * value[0]\n",
    "        ce_loss /= len(labels_test)\n",
    "        data = {\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"test_precision\": precision,\n",
    "            \"test_recall\": recall,\n",
    "            \"F_score\": 2 * precision * recall / (precision + recall),\n",
    "            \"mse_loss\": np.sum(np.array(labels_test) - model.predict_proba(test_features)[:, 1]) ** 2 / len(labels_test),\n",
    "            \"ce_loss\": ce_loss,\n",
    "            \"param\": search.best_params_,\n",
    "            \"label_and_result\": list(zip(labels_test, model.predict(test_features).tolist(), result_proba[:, 1].tolist())),\n",
    "        }\n",
    "        print(data)\n",
    "        json.dump(data, open(os.path.join(log_dir, \"result{}.json\".format(idx)), \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_accuracy': 85.83333333333333, 'test_precision': 0.9122807017543859, 'test_recall': 0.8125, 'F_score': 0.859504132231405, 'mse_loss': 0.5380300978392314, 'ce_loss': 0.4264432430521929, 'param': {'C': 64}, 'label_and_result': [(1, 1, 0.5632702353288629), (0, 0, 0.19871362267397785), (0, 0, 0.12707697698564305), (0, 0, 0.01617545614247215), (0, 0, 0.24381743954360924), (1, 1, 0.8835580692029219), (1, 1, 0.5609684354253088), (0, 0, 0.2700504124219628), (1, 1, 0.8929476844287356), (0, 0, 0.03227692911773108), (0, 0, 0.1307951205593235), (1, 0, 0.22470776025905675), (0, 0, 0.34078161291801656), (1, 1, 0.9607576738825248), (0, 0, 0.07432146444400861), (1, 1, 0.6506884584456407), (1, 1, 0.6563874795070928), (1, 1, 0.8337588128354759), (1, 1, 0.8412751894419865), (1, 0, 0.23824859781465577), (1, 1, 0.6184054848628312), (0, 0, 0.1597761865832577), (0, 0, 0.18779994738087652), (1, 1, 0.9528871641866076), (1, 0, 0.44264756058889776), (0, 0, 0.251052431033892), (1, 1, 0.9218332001720144), (0, 0, 0.4587369498431008), (1, 1, 0.857348115525492), (0, 0, 0.42407253072271855), (0, 0, 0.059502513375841504), (0, 0, 0.18562437487242733), (1, 1, 0.8570518900962846), (0, 1, 0.5070036382414578), (1, 0, 0.37872116979185333), (0, 0, 0.3051604734011812), (1, 1, 0.5325081476204128), (0, 0, 0.15677974733885727), (0, 0, 0.018466790767460193), (0, 0, 0.1420636804156376), (1, 1, 0.7242069712130406), (0, 0, 0.43998803470955394), (1, 1, 0.9603802504500757), (1, 1, 0.9524109978487931), (1, 1, 0.7509595405377253), (1, 1, 0.8902070044285264), (0, 0, 0.08894956460939474), (1, 1, 0.6256033615479951), (0, 0, 0.028172748704564363), (1, 1, 0.6419238459295403), (0, 0, 0.08843936295020431), (1, 0, 0.041192106076459976), (0, 0, 0.03376195953859741), (0, 0, 0.27984947672791866), (0, 0, 0.32986553996022816), (1, 1, 0.8474365167164276), (0, 0, 0.49278186905885435), (0, 0, 0.065197737407866), (0, 0, 0.0798148065720527), (0, 0, 0.3983940973602097), (1, 0, 0.11764895116554855), (1, 1, 0.8464357427766019), (0, 1, 0.6461654952215375), (1, 1, 0.9393524652728328), (1, 1, 0.8992101883493755), (1, 1, 0.985670980857226), (0, 0, 0.08866214246014263), (0, 0, 0.49661181916654795), (0, 0, 0.011542318586571057), (1, 1, 0.8611210736952161), (1, 1, 0.5141312395263371), (1, 1, 0.9183783573242336), (0, 0, 0.02213518352717035), (1, 1, 0.7940430352073378), (0, 0, 0.1661320731951267), (0, 1, 0.7373696909493267), (1, 1, 0.9391200384738848), (1, 1, 0.7978340821255303), (0, 0, 0.11039743793692672), (1, 1, 0.8448741402273535), (1, 0, 0.16709216215133), (1, 1, 0.8734728396866334), (1, 0, 0.4896334470288787), (0, 0, 0.058765687262437756), (0, 0, 0.08308275549261755), (0, 0, 0.11403142572862744), (0, 0, 0.05728396146516532), (1, 1, 0.8399037313256029), (1, 1, 0.6063193810358399), (1, 0, 0.039235644671482904), (0, 0, 0.2133813205036259), (1, 1, 0.751091563955277), (1, 1, 0.633380520561346), (1, 1, 0.966670781830971), (0, 0, 0.05408562042738114), (1, 1, 0.5179123519671578), (1, 1, 0.7111701318650843), (0, 0, 0.3740244114927129), (1, 1, 0.6523752132236483), (0, 1, 0.5348054307140537), (1, 1, 0.9148800419151425), (1, 1, 0.6273918048478244), (1, 0, 0.3194399968210063), (0, 0, 0.2426974781767056), (0, 0, 0.15564773506604532), (1, 0, 0.08295698359796343), (1, 1, 0.7664320857785065), (0, 0, 0.11024621060109156), (1, 1, 0.8854193063407242), (1, 1, 0.6805415225153064), (0, 1, 0.5631902189166434), (1, 0, 0.17418335477473995), (0, 0, 0.3959336885022983), (1, 1, 0.7132226492869097), (0, 0, 0.31209721361495607), (0, 0, 0.4395984564731764), (1, 1, 0.7571874266859484), (1, 1, 0.516004555796099), (1, 1, 0.8920381358127804), (0, 0, 0.02363255535331037)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\w2v_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\w2v_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\w2v_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\w2v_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "e:\\스꾸\\4-1\\인공지능프로젝트-박호건\\aip\\w2v_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# run main function\n",
    "\n",
    "main(train_test_ratio=0.2)\n",
    "# main_overfit(train_test_ratio=0.2)\n",
    "# main_kfold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
